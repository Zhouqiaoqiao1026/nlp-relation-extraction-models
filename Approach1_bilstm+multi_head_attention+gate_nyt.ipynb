{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fohOFdVhp6i2"
   },
   "source": [
    "# RE Approach 1: BiLSTM+Multi-head attention+ Dynamic gate \n",
    "\n",
    "# NOTE:  To load the best trained model(saved in the same folder) and test it, please run the LAST 2 cells !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BvaYuPiup6i5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "#If run with Google Colab, uncomment the following\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qEikCrjeLA8R"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 1: Data Processing - Extracting relationships\n",
    "#-----------------------------------------------------\n",
    "def extract_relations(json_line):\n",
    "    relations = []\n",
    "    sent_text = json_line['sentText']\n",
    "    for relation in json_line['relationMentions']:\n",
    "        em1_text = relation['em1Text']\n",
    "        em2_text = relation['em2Text']\n",
    "        label = relation['label']\n",
    "        relations.append((sent_text, em1_text, em2_text, label))\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DTve1HpVp6i6"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 2: Read the dataset and process the data\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#If you need to load the dataset, replace the local file path with the following\n",
    "train_input_file = './NYT11/trainNTY11.json'\n",
    "valid_input_file = './NYT11/validNTY11.json'\n",
    "test_input_file = './NYT11/testNTY11.json'\n",
    "\n",
    "#If run with Google Colab, uncomment the following:\n",
    "#train_input_file = '/content/drive/MyDrive/trainNTY11.json'\n",
    "#valid_input_file = '/content/drive/MyDrive/validNTY11.json'\n",
    "#test_input_file = '/content/drive/MyDrive/testNTY11.json'\n",
    "\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "with open(train_input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        json_line = json.loads(line)\n",
    "        relations = extract_relations(json_line)\n",
    "        train_data.extend(relations)\n",
    "train_df = pd.DataFrame(train_data, columns=['sentence', 'entity1', 'entity2', 'label'])\n",
    "\n",
    "\n",
    "with open(valid_input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        json_line = json.loads(line)\n",
    "        relations = extract_relations(json_line)\n",
    "        val_data.extend(relations)\n",
    "valid_df = pd.DataFrame(val_data, columns=['sentence', 'entity1', 'entity2', 'label'])\n",
    "\n",
    "with open(test_input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        json_line = json.loads(line)\n",
    "        relations = extract_relations(json_line)\n",
    "        test_data.extend(relations)\n",
    "test_df = pd.DataFrame(test_data, columns=['sentence', 'entity1', 'entity2', 'label'])\n",
    "\n",
    "\n",
    "# Change the markup to a uniform format (remove the first character and replace/with _)\n",
    "#train_df['label'] = train_df['label'].str[1:]\n",
    "#valid_df['label'] = valid_df['label'].str[1:]\n",
    "#test_df['label'] = test_df['label'].str[1:]\n",
    "train_df['label'] = train_df['label'].str.replace('/', '_')\n",
    "valid_df['label'] = valid_df['label'].str.replace('/', '_')\n",
    "test_df['label'] = test_df['label'].str.replace('/', '_')\n",
    "\n",
    "# Change all data types to strings\n",
    "train_df = train_df.astype(str)\n",
    "valid_df = valid_df.astype(str)\n",
    "test_df = test_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X71PNY-Tp6i6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'None': 0, '_location_location_contains': 1, '_location_administrative_division_country': 2, '_location_country_administrative_divisions': 3, '_location_country_capital': 4, '_people_person_children': 5, '_people_person_place_lived': 6, '_people_person_nationality': 7, '_business_company_place_founded': 8, '_location_neighborhood_neighborhood_of': 9, '_people_person_place_of_birth': 10, '_sports_sports_team_location': 11, '_sports_sports_team_location_teams': 12, '_people_deceased_person_place_of_death': 13, '_business_company_founders': 14, '_business_person_company': 15, '_business_company_major_shareholders': 16, '_business_company_shareholder_major_shareholder_of': 17, '_people_ethnicity_people': 18, '_people_person_ethnicity': 19, '_business_company_advisors': 20, '_people_person_religion': 21, '_people_ethnicity_geographic_distribution': 22, '_people_person_profession': 23, '_business_company_industry': 24}\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "# 3: Data preprocessing functions\n",
    "#-----------------------------------------------------\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['label'].unique())}\n",
    "max_length = 96 \n",
    "print(label_encoder)\n",
    "def preprocess_data(row):\n",
    "    sentence = row['sentence']\n",
    "    entity1 = row['entity1']\n",
    "    entity2 = row['entity2']\n",
    "\n",
    "    # Tokenize sentence，get tokenized 后的 input_ids\n",
    "    encoded_sentence = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_sentence['input_ids'].squeeze()\n",
    "    attention_mask = encoded_sentence['attention_mask'].squeeze()\n",
    "\n",
    "    # Get the tokenized entity location\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    entity1_tokens = tokenizer.tokenize(entity1)\n",
    "    entity2_tokens = tokenizer.tokenize(entity2)\n",
    "\n",
    "    entity1_pos = [i for i, token in enumerate(tokenized_text) if token in entity1_tokens]\n",
    "    entity2_pos = [i for i, token in enumerate(tokenized_text) if token in entity2_tokens]\n",
    "\n",
    "    # Prevent boundaries\n",
    "    entity1_pos = min(entity1_pos[0] if entity1_pos else 0, max_length - 1)\n",
    "    entity2_pos = min(entity2_pos[0] if entity2_pos else 0, max_length - 1)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'entity1_pos': torch.tensor(entity1_pos),\n",
    "        'entity2_pos': torch.tensor(entity2_pos),\n",
    "        'label': torch.tensor(label_encoder[row['label']])\n",
    "    }\n",
    "train_data = train_df.apply(preprocess_data, axis=1).tolist()\n",
    "valid_data = valid_df.apply(preprocess_data, axis=1).tolist()\n",
    "test_data = test_df.apply(preprocess_data, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RYGfi1XFp6i6"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 4. Custom dataset class function definition\n",
    "#-----------------------------------------------------\n",
    "class RelationshipDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = RelationshipDataset(train_data)\n",
    "valid_dataset = RelationshipDataset(valid_data)\n",
    "test_dataset = RelationshipDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QvKwpoaqp6i7"
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "#-----------------------------------------------------\n",
    "# 5:【Optimization part】Gating unit design function\n",
    "\n",
    "# Note: \n",
    "# This DynamicGate module dynamically combines two features (original and transformed) by using a gating mechanism. It uses a linear layer followed by a Sigmoid activation to calculate the gating coefficients, which determine how much of each feature contributes to the final output.\n",
    "#      Gate Layer: Generates coefficients to control the fusion of features.\n",
    "#      Forward Pass: Concatenates the features, calculates the gate, and performs weighted fusion.\n",
    "# The module allows the model to adaptively adjust the influence of each feature based on the input.\n",
    "#-----------------------------------------------------\n",
    "#*****************************************************\n",
    "\n",
    "class DynamicGate(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "       # Gated network design (using linear layers to generate conditioning coefficients)\n",
    "        self.gate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 4, hidden_size * 2),  # Enter concatenated double dimensions\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, original, transformed):\n",
    "        # Concatenate both features as gating input\n",
    "        combined = torch.cat([original, transformed], dim=-1)  # [batch, seq_len, hidden*4]\n",
    "\n",
    "        # Generate dynamic gating coefficients\n",
    "        gate = self.gate_layer(combined)  # [batch, seq_len, hidden*2]\n",
    "\n",
    "        # Gated fusion (element-wise adjustment)）\n",
    "        return gate * original + (1 - gate) * transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "N0BsJr19DuVe"
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "#-----------------------------------------------------\n",
    "# 6: 【Optimization part】Overall model design function: BiLSTMModel+ Multi-head Attention + Dynamic gate\n",
    "\n",
    "# Note: \n",
    "# The BiLSTM (Bidirectional LSTM) layer processes input data in both forward and backward directions, allowing \n",
    "# the model to capture information from the past and future of a sequence, which is helpful for tasks like sequence classification.\n",
    "\n",
    "# The Multi-head Attention layer focuses on different parts of the input sequence, learning which parts are most \n",
    "# important for the task. It assigns different attention weights to different parts of the sequence, helping the model focus on relevant information.\n",
    "#-----------------------------------------------------\n",
    "#*****************************************************\n",
    "\n",
    "import torch\n",
    "\n",
    "class BiLSTMModelWithAttention(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, num_classes):\n",
    "        super(BiLSTMModelWithAttention, self).__init__()\n",
    "        # 1. embedding layer\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # 2. BiLSTM layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        # 3. Multi-head Attention layer\n",
    "        self.multihead_attn = torch.nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True)\n",
    "\n",
    "        # 4. Dynamic gate layer\n",
    "        self.dynamic_gate = DynamicGate(hidden_size)\n",
    "\n",
    "       \n",
    "        # Modify MLP input size dynamically based on hidden_size\n",
    "        mlp_input_dim = hidden_size * 10  # Adjust based on actual concatenation\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(mlp_input_dim, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, entity1_pos, entity2_pos):\n",
    "        # Get embedding representation [batch_size, seq_len, hidden_size]\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # BiLSTM output [batch_size, seq_len, hidden_size*2]\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        # Handle attention mask\n",
    "        key_padding_mask = (attention_mask == 0)  # [batch_size, seq_len]\n",
    "\n",
    "        # Multi-head Attention Computation [batch_size, seq_len, hidden_size*2]\n",
    "        attn_out, attn_weights = self.multihead_attn(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            average_attn_weights=False)\n",
    "\n",
    "        # Compute dynamic gate weights\n",
    "        combined = torch.cat([lstm_out, attn_out], dim=-1)  # [batch, seq_len, hidden*4]\n",
    "        gate_weights = self.dynamic_gate.gate_layer(combined)\n",
    "\n",
    "        # Apply dynamic gating\n",
    "        refined_features = self.dynamic_gate(lstm_out, attn_out)\n",
    "\n",
    "        # Extract entity features\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        # Ensure the index does not exceed the maximum range\n",
    "        # Ensure that the index does not exceed the maximum range\n",
    "        entity1_pos = torch.min(entity1_pos, torch.tensor(max_length-1))\n",
    "        entity2_pos = torch.min(entity2_pos, torch.tensor(max_length-1))\n",
    "\n",
    "        entity1_hidden = refined_features[torch.arange(batch_size), entity1_pos]  # [batch, hidden*2]\n",
    "        entity2_hidden = refined_features[torch.arange(batch_size), entity2_pos]  # [batch, hidden*2]\n",
    "\n",
    "        # Compute additional features\n",
    "        feature_diff = torch.abs(entity1_hidden - entity2_hidden)\n",
    "        feature_mul = entity1_hidden * entity2_hidden\n",
    "\n",
    "        # Concatenate all features [batch, hidden_size*5]\n",
    "        combined = torch.cat([entity1_hidden, entity2_hidden, feature_diff, feature_mul, refined_features.mean(dim=1)], dim=1)\n",
    "\n",
    "        # Classification through MLP\n",
    "        logits = self.mlp(combined)\n",
    "\n",
    "        return logits, attn_weights, gate_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_r023QgJp6i7"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 7. Set up the training loop\n",
    "#-----------------------------------------------------\n",
    "\n",
    "num_labels = len(train_df['label'].unique())\n",
    "\n",
    "input_size = len(tokenizer.vocab)   # Vocabulary size\n",
    "embedding_size = 256  # Dimension of word embedding\n",
    "hidden_size = 256     # LSTM hidden layer dimension (bidirectional total dimension)\n",
    "num_layers = 2\n",
    "num_classes = num_labels\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 5\n",
    "\n",
    "# Initializing the model\n",
    "bi_lstm_model = BiLSTMModelWithAttention(\n",
    "    vocab_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=16,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(bi_lstm_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-VzkB8kIp6i8"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 8. Train and evaluate function definitions\n",
    "#-----------------------------------------------------\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            entity1_pos = batch['entity1_pos'].to(device)\n",
    "            entity2_pos = batch['entity2_pos'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits, attn_weights, gate_weights = model(input_ids, attention_mask, entity1_pos, entity2_pos)\n",
    "\n",
    "            # Use torch.max only for logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            total_preds.extend(preds.cpu().tolist())\n",
    "            total_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return accuracy_score(total_labels, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "62ba6092b30f4a55837ef04da55cd684"
     ]
    },
    "id": "0j2AmWCPDuVg",
    "outputId": "f2c0a470-b75e-440c-aa81-83d181bf98b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0116e7db164cdeafaabe8cfbc5aa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/20991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Train Accuracy: 0.8672. validation Accuracy: 0.8516\n",
      "Best model saved with accuracy: 0.8516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdb5a3421a6479fa90789d0ea20275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/20991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Train Accuracy: 0.8963. validation Accuracy: 0.8715\n",
      "Best model saved with accuracy: 0.8715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e034bf1fe5b548868d904b4b2a63ebda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/20991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Train Accuracy: 0.9159. validation Accuracy: 0.8817\n",
      "Best model saved with accuracy: 0.8817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79a2610d40f4ea6aeeee23fcd21bc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/20991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Train Accuracy: 0.9254. validation Accuracy: 0.8836\n",
      "Best model saved with accuracy: 0.8836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d285cc81afb34e3295c8c5db80350305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/20991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Train Accuracy: 0.9336. validation Accuracy: 0.8842\n",
      "Best model saved with accuracy: 0.8842\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "# 9. Training and evaluation process\n",
    "#-----------------------------------------------------\n",
    "\n",
    "best_valid_accuracy = 0.0\n",
    "best_model_path = 'best_BiLSTMmodelmlp.pth'  # save best model path\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bi_lstm_model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        entity1_pos = batch['entity1_pos'].to(device)\n",
    "        entity2_pos = batch['entity2_pos'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass including position parameters\n",
    "        #Note:attn_weights, gate_weights are not used, just reserved for printing the progress of the process if is needed.\n",
    "        logits, attn_weights, gate_weights = bi_lstm_model(input_ids, attention_mask, entity1_pos, entity2_pos)\n",
    "\n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "    # Calculate accuracy for Train and Valid set\n",
    "    train_accuracy = evaluate(bi_lstm_model, train_dataloader)\n",
    "    valid_accuracy = evaluate(bi_lstm_model, valid_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Train Accuracy: {train_accuracy:.4f}. validation Accuracy: {valid_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model if validation accuracy is better than previous\n",
    "    if valid_accuracy \n",
    "    > best_valid_accuracy:\n",
    "        best_valid_accuracy = valid_accuracy\n",
    "        torch.save(bi_lstm_model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved with accuracy: {best_valid_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# 10. Assessment module function\n",
    "#-----------------------------------------------------\n",
    "\n",
    "relation_cls_label_map = {\n",
    "    0: 'None',\n",
    "    1: '_location_location_contains',\n",
    "    2: '_location_administrative_division_country',\n",
    "    3: '_location_country_administrative_divisions',\n",
    "    4: '_location_country_capital',\n",
    "    5: '_people_person_children',\n",
    "    6: '_people_person_place_lived',\n",
    "    7: '_people_person_nationality',\n",
    "    8: '_business_company_place_founded',\n",
    "    9: '_location_neighborhood_neighborhood_of',\n",
    "    10: '_people_person_place_of_birth',\n",
    "    11: '_sports_sports_team_location',\n",
    "    12: '_sports_sports_team_location_teams',\n",
    "    13: '_people_deceased_person_place_of_death',\n",
    "    14: '_business_company_founders',\n",
    "    15: '_business_person_company',\n",
    "    16: '_business_company_major_shareholders',\n",
    "    17: '_business_company_shareholder_major_shareholder_of',\n",
    "    18: '_people_ethnicity_people',\n",
    "    19: '_people_person_ethnicity',\n",
    "    20: '_business_company_advisors',\n",
    "    21: '_people_person_religion',\n",
    "    22: '_people_ethnicity_geographic_distribution',\n",
    "    23: '_people_person_profession',\n",
    "    24: '_business_company_industry'\n",
    "}\n",
    "\n",
    "\n",
    "# Categories to ignore (usually the \"unrelated\" category)\n",
    "ignore_rel_list = ['None']\n",
    "def get_threshold(data, preds):\n",
    "    max_f1 = -1.0\n",
    "    best_th = -1.0\n",
    "    cur_th = 0.0\n",
    "\n",
    "    while cur_th < 1.0:\n",
    "        pred_pos, gt_pos, correct_pos, total_correct, total_samples= get_F1(data, preds, threshold=cur_th)\n",
    "        p = float(correct_pos) / (pred_pos + 1e-8)\n",
    "        r = float(correct_pos) / (gt_pos + 1e-8)\n",
    "        cur_f1 = (2 * p * r) / (p + r + 1e-8)\n",
    "\n",
    "        if cur_f1 > max_f1:\n",
    "            max_f1 = cur_f1\n",
    "            best_th = cur_th\n",
    "        cur_th += 0.01  # The best threshold was searched with a step size of 0.01\n",
    "\n",
    "    return best_th\n",
    "\n",
    "def get_F1(data, preds_probs, threshold=0.0):\n",
    "    gt_pos = 0\n",
    "    pred_pos = 0\n",
    "    correct_pos = 0\n",
    "    total_correct = 0  \n",
    "    total_samples = len(data) \n",
    "\n",
    "    for i in range(len(preds_probs)):\n",
    "        true_label_idx = data[i]['label'].item()\n",
    "        org_rel_name = relation_cls_label_map[true_label_idx]\n",
    "        pred_val = np.argmax(preds_probs[i])\n",
    "        pred_rel_name = relation_cls_label_map[pred_val]\n",
    "        max_prob = np.max(preds_probs[i])\n",
    "\n",
    "        # Adjusted prediction: If the prediction is non-None and the probability > threshold is retained, otherwise it is treated as None\n",
    "        if pred_rel_name not in ignore_rel_list and max_prob > threshold:\n",
    "            adjusted_pred = pred_val\n",
    "        else:\n",
    "            adjusted_pred = 0  # the index\n",
    "\n",
    "\n",
    "        if adjusted_pred == true_label_idx:\n",
    "            total_correct += 1\n",
    "\n",
    "        if org_rel_name not in ignore_rel_list:\n",
    "            gt_pos += 1\n",
    "        if (pred_rel_name not in ignore_rel_list) and (max_prob > threshold):\n",
    "            pred_pos += 1\n",
    "        if (org_rel_name == pred_rel_name) and (org_rel_name not in ignore_rel_list) and (max_prob > threshold):\n",
    "            correct_pos += 1\n",
    "\n",
    "    return pred_pos, gt_pos, correct_pos, total_correct, total_samples\n",
    "\n",
    "def evaluate_metricsNew(data, preds_probs, threshold=0.0):\n",
    "    pred_pos, gt_pos, correct_pos, total_correct, total_samples = get_F1(data, preds_probs, threshold)\n",
    "\n",
    "    precision = correct_pos / (pred_pos + 1e-8)\n",
    "    recall = correct_pos / (gt_pos + 1e-8)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-8)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    # Outputting each metric\n",
    "    print(f\"Threshold = {threshold:.2f}\")\n",
    "    print(f\"Accuracy = {accuracy:.4f}\")\n",
    "    print(f\"Precision = {precision:.4f}\")\n",
    "    print(f\"Recall = {recall:.4f}\")\n",
    "    print(f\"F1 = {f1:.4f}\")\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "def evaluate_metric_modelNew(model, dataloader, dataset, threshold=0.0):\n",
    "    model.eval()\n",
    "    all_pred_probs = []\n",
    "    all_valid_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            entity1_pos = batch['entity1_pos'].to(device)\n",
    "            entity2_pos = batch['entity2_pos'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, entity1_pos, entity2_pos)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            all_pred_probs.extend(probs)\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            entity1_pos = batch['entity1_pos'].to(device)\n",
    "            entity2_pos = batch['entity2_pos'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, entity1_pos, entity2_pos)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            all_valid_probs.extend(probs)\n",
    "\n",
    "    if len(all_pred_probs) != len(dataset):\n",
    "        raise ValueError(f\"The number of predictions ({len(all_pred_probs)}) does not match the number of dataset samples({len(dataset)})\")\n",
    "    threshold = get_threshold(valid_dataset, all_valid_probs)\n",
    "    #print(threshold)\n",
    "\n",
    "    # Call the evaluation function and return the accuracy\n",
    "    precision, recall, f1, accuracy = evaluate_metricsNew(dataset, all_pred_probs, threshold)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/nvhbd9497sq4mxjs1jnndqy40000gn/T/ipykernel_53960/3568392529.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bi_lstm_model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.42\n",
      "Accuracy = 0.5662\n",
      "Precision = 0.4171\n",
      "Recall = 0.6385\n",
      "F1 = 0.5046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#-----------------------------------------------------\n",
    "# 11. Load the best trained model and output the score of test set\n",
    "#-----------------------------------------------------\n",
    "\n",
    "bi_lstm_model.load_state_dict(torch.load(best_model_path))\n",
    "bi_lstm_model.eval()\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_metric_modelNew(bi_lstm_model, test_dataloader, test_dataset, threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------END----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load the best trained model(saved in the same folder) and test it, please run the following 2 cells !  You can input your test cases in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pandas ...\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Installing torch ...\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Installing transformers ...\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0\n",
      "tqdm Installed\n",
      "Installing scikit-learn ...\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "numpy Installed\n",
      "Installing gdown ...\n",
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/bert/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1w7zZ1a3-nOzq_Vb3YvJyoJAVp1e_H5NQ\n",
      "From (redirected): https://drive.google.com/uc?id=1w7zZ1a3-nOzq_Vb3YvJyoJAVp1e_H5NQ&confirm=t&uuid=4bf226e4-c3f9-465f-a440-96a7c964eea7\n",
      "To: /Users/zhouqiaoqiao/Desktop/【61332】 Text Mining/CW最终提交材料/best_BiLSTMmodel.pth\n",
      "100%|██████████| 53.9M/53.9M [00:02<00:00, 23.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded successfully!\n",
      "BiLSTM Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "# 12: For teachers to test our model\n",
    "#-----------------------------------------------------\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of packages to install\n",
    "required_packages = [\"pandas\", \"torch\", \"transformers\", \"tqdm\", \"scikit-learn\", \"numpy\", \"gdown\"]\n",
    "\n",
    "# Install the missing libraries\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} Installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package} ...\")\n",
    "        install(package)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Load the model and related variables\n",
    "#-----------------------------------------------------\n",
    "import torch\n",
    "import os\n",
    "import gdown\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "# Google Drive File link\n",
    "file_id = \"1w7zZ1a3-nOzq_Vb3YvJyoJAVp1e_H5NQ\"\n",
    "best_model_path = \"./best_BiLSTMmodel.pth\"  # Local storage path\n",
    "\n",
    "# If the model file does not exist, it is downloaded\n",
    "if not os.path.exists(best_model_path):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", best_model_path, quiet=False)\n",
    "    print(\"Model downloaded successfully!\")\n",
    "\n",
    "\n",
    "#Related parameter introduction\n",
    "num_labels=25\n",
    "input_size = len(tokenizer.vocab)   # Vocabulary size\n",
    "embedding_size = 256  # Dimension of word embedding\n",
    "hidden_size = 256     # LSTM hidden layer dimension (bidirectional total dimension)\n",
    "num_layers = 2\n",
    "max_length=96\n",
    "num_classes = num_labels\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "label_encoder = {'None': 0, 'location_location_contains': 1, 'location_administrative_division_country': 2, 'location_country_administrative_divisions': 3, 'location_country_capital': 4, 'people_person_children': 5, 'people_person_place_lived': 6, 'people_person_nationality': 7, 'business_company_place_founded': 8, 'location_neighborhood_neighborhood_of': 9, 'people_person_place_of_birth': 10, 'sports_sports_team_location': 11, 'sports_sports_team_location_teams': 12, 'people_deceased_person_place_of_death': 13, 'business_company_founders': 14, 'business_person_company': 15, 'business_company_major_shareholders': 16, 'business_company_shareholder_major_shareholder_of': 17, 'people_ethnicity_people': 18, 'people_person_ethnicity': 19, 'business_company_advisors': 20, 'people_person_religion': 21, 'people_ethnicity_geographic_distribution': 22, 'people_person_profession': 23, 'business_company_industry': 24}\n",
    "\n",
    "#Related Functions introduction\n",
    "class DynamicGate(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "       # Gated network design (using linear layers to generate conditioning coefficients)\n",
    "        self.gate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 4, hidden_size * 2),  # Enter concatenated double dimensions\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, original, transformed):\n",
    "        # Concatenate both features as gating input\n",
    "        combined = torch.cat([original, transformed], dim=-1)  # [batch, seq_len, hidden*4]\n",
    "\n",
    "        # Generate dynamic gating coefficients\n",
    "        gate = self.gate_layer(combined)  # [batch, seq_len, hidden*2]\n",
    "\n",
    "        # Gated fusion (element-wise adjustment)）\n",
    "        return gate * original + (1 - gate) * transformed\n",
    "\n",
    "class BiLSTMModelWithAttention(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, num_classes):\n",
    "        super(BiLSTMModelWithAttention, self).__init__()\n",
    "        # 1. embedding layer\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # 2. BiLSTM layer\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        # 3. Multi-head Attention layer\n",
    "        self.multihead_attn = torch.nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True)\n",
    "\n",
    "        # 4. Dynamic gate layer\n",
    "        self.dynamic_gate = DynamicGate(hidden_size)\n",
    "\n",
    "       \n",
    "        # Modify MLP input size dynamically based on hidden_size\n",
    "        mlp_input_dim = hidden_size * 10  # Adjust based on actual concatenation\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(mlp_input_dim, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, entity1_pos, entity2_pos):\n",
    "        # Get embedding representation [batch_size, seq_len, hidden_size]\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # BiLSTM output [batch_size, seq_len, hidden_size*2]\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        # Handle attention mask\n",
    "        key_padding_mask = (attention_mask == 0)  # [batch_size, seq_len]\n",
    "\n",
    "        # Multi-head Attention Computation [batch_size, seq_len, hidden_size*2]\n",
    "        attn_out, attn_weights = self.multihead_attn(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            average_attn_weights=False)\n",
    "\n",
    "        # Compute dynamic gate weights\n",
    "        combined = torch.cat([lstm_out, attn_out], dim=-1)  # [batch, seq_len, hidden*4]\n",
    "        gate_weights = self.dynamic_gate.gate_layer(combined)\n",
    "\n",
    "        # Apply dynamic gating\n",
    "        refined_features = self.dynamic_gate(lstm_out, attn_out)\n",
    "\n",
    "        # Extract entity features\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        # Ensure the index does not exceed the maximum range\n",
    "        # Ensure that the index does not exceed the maximum range\n",
    "        entity1_pos = torch.min(entity1_pos, torch.tensor(max_length-1))\n",
    "        entity2_pos = torch.min(entity2_pos, torch.tensor(max_length-1))\n",
    "\n",
    "        entity1_hidden = refined_features[torch.arange(batch_size), entity1_pos]  # [batch, hidden*2]\n",
    "        entity2_hidden = refined_features[torch.arange(batch_size), entity2_pos]  # [batch, hidden*2]\n",
    "\n",
    "        # Compute additional features\n",
    "        feature_diff = torch.abs(entity1_hidden - entity2_hidden)\n",
    "        feature_mul = entity1_hidden * entity2_hidden\n",
    "\n",
    "        # Concatenate all features [batch, hidden_size*5]\n",
    "        combined = torch.cat([entity1_hidden, entity2_hidden, feature_diff, feature_mul, refined_features.mean(dim=1)], dim=1)\n",
    "\n",
    "        # Classification through MLP\n",
    "        logits = self.mlp(combined)\n",
    "\n",
    "        return logits, attn_weights, gate_weights\n",
    "    \n",
    "# Load saved optimal model\n",
    "def load_best_model(model, model_path):\n",
    "    #model.load_state_dict(torch.load(model_path))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))) \n",
    "    model.eval()\n",
    "    print(f\"BiLSTM Model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "#Function definition for relation prediction on an input sentence and two entities using the loaded model\n",
    "def predict_relationship_with_saved_model(sentence, entity1, entity2, model, tokenizer, label_encoder, max_length=256):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    entity1_pos = [i for i, token in enumerate(tokens) if token in tokenizer.tokenize(entity1)]\n",
    "    entity2_pos = [i for i, token in enumerate(tokens) if token in tokenizer.tokenize(entity2)]\n",
    "\n",
    "    # tokenize the input sentence\n",
    "    encoded_sentence = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "    input_ids = encoded_sentence['input_ids'].to(device)\n",
    "    attention_mask = encoded_sentence['attention_mask'].to(device)\n",
    "    entity1_pos = torch.tensor(entity1_pos[0] if entity1_pos else 0).unsqueeze(0).to(device)\n",
    "    entity2_pos = torch.tensor(entity2_pos[0] if entity2_pos else 0).unsqueeze(0).to(device)\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, entity1_pos, entity2_pos)\n",
    "        logits = outputs[0]\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "    # The labels are obtained by reverse mapping\n",
    "    reverse_label_encoder = {v: k for k, v in label_encoder.items()}\n",
    "    predicted_label = reverse_label_encoder[preds.item()]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "bi_lstm_model = BiLSTMModelWithAttention(\n",
    "    vocab_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=16,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# load best model\n",
    "bi_lstm_model = load_best_model(bi_lstm_model, best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted relationships：people_person_children\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "# 13: Input Module： Please input your test case here!\n",
    "#-----------------------------------------------------\n",
    "\n",
    "# The sentence and two entities (entity1 and entity2) are provided as input for relationship prediction.\n",
    "# Users can modify these variables to test the model with different sentences and entities.\n",
    "\n",
    "# In this example, 'sentence' is a string that represents the sentence where the entities appear.\n",
    "# 'entity1' and 'entity2' are the two entities whose relationship you want to predict. You can replace \n",
    "# these with any sentence and entities of your choice.\n",
    "\n",
    "# To test the module:\n",
    "# 1. Modify the 'sentence' variable with your desired sentence that contains two entities.\n",
    "# 2. Modify 'entity1' with the first entity (a person, organization, or any other entity) in the sentence.\n",
    "# 3. Modify 'entity2' with the second entity (the relationship you want to identify between entity1 and entity2).\n",
    "\n",
    "# After modifying these values, the model will predict the relationship between the two entities in the sentence.\n",
    "# The predicted relationship will be printed out.\n",
    "\n",
    "\n",
    "sentence = \"otecna employed Kojo Annan , Kofi Annan 's son , as a contractor at the time it received the aid inspection contract\"\n",
    "entity1 = \"Kojo Annan\"\n",
    "entity2 = \"Kofi Annan\"\n",
    "\n",
    "# Relationship prediction using the loaded model\n",
    "predicted_relationship = predict_relationship_with_saved_model(sentence, entity1, entity2, bi_lstm_model, tokenizer, label_encoder)\n",
    "print(f\"Predicted relationships：{predicted_relationship}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1504204,
     "sourceId": 2485002,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
